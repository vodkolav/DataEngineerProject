{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox  frim twitter producer\n",
    "below are cells that I played with to test portions of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "int.encode(3423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir(str.encode('434234'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = str.encode('434g34')\n",
    "wat.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 99991231235\n",
    "#i=1\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode = lambda x : x.to_bytes(5, byteorder='big')\n",
    "decode = lambda x : int.from_bytes(x, byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = encode(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(encode(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decode(encode(i)) == i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int.from_bytes(hm, byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(hm, base =2, byteorder='big')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traceback.extract_tb(oops.__traceback__ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datefmt = \"%a %b %d %H:%M:%S %z %Y\"\n",
    "\n",
    "date = \"Sun Jan 13 23:03:23 +0000 2019\"\n",
    "\n",
    "def date_fmt_convert(date, fmt = \"%Y%m%d%H%M\"):        \n",
    "    # Convert twitter datetime format such as \n",
    "    # \"Sat Jan 4 11:39:13 +0500 2019\"\n",
    "    # to partitioning-compatible format and shift it to utc, so: \n",
    "    # int: 201901040639, int: \n",
    "    date_time_obj = datetime.strptime(date, datefmt)\n",
    "    date_time_obj.astimezone(tz=timezone.utc)\n",
    "    return int(date_time_obj.strftime(fmt)), int(date_time_obj.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_fmt_convert(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = stream.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata = stream.rawdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = tweets\n",
    "print(len(wat))\n",
    "wat4 = wat[1]\n",
    "wat4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat4[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat4 = wat4.replace('1210547837035073536', '12105478370350735364305983475023948606723056927602')\n",
    "wat4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watj = json.loads(wat4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = wat4[1]['user']['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.loads(wat[i])['extended_tweet']['full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(json.loads(wat[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    date_time_obj = datetime.strptime(date, self.datefmt)\n",
    "        date_time_obj = date_time_obj.astimezone(tz=timezone.utc)\n",
    "        return date_time_obj.strftime(\"%Y-%m-%d_%H-%M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = wat4['tweet']['created_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " date_time_obj = datetime.strptime(date, \"%Y-%m-%d_%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(date_time_obj.timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_event = json.loads(wat4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_event['retweeted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_event = json.dumps(api_event, ensure_ascii=False)\n",
    "str_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test.json','w') as f:\n",
    "    str_event = json.dump(api_event,f, ensure_ascii=False)\n",
    "str_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bytes(str_event, 'utf-8' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat[4]['retweeted_status']#['extended_tweet']['full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for playing with code snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tweepy\n",
    "from datetime import datetime, timezone\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_serializer=lambda v: json.dumps(v).encode()\n",
    "value_serializer(json.dumps('afgsfgsdfg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = \"Sat Jan 4 11:39:13 +0500 2019\"\n",
    "fmt = \"%a %b %d %H:%M:%S %z %Y\"\n",
    "date_time_obj = datetime.strptime(dat, fmt)\n",
    "(date_time_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_obj = date_time_obj.astimezone(tz=timezone.utc)\n",
    "date_time_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time_obj.strftime(\"%Y-%m-%d_%H-%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ified fields.\n",
    "Type:      builtin_function_or_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Jun 28 2018 at 7:40AM\" -> \"%b %d %Y at %I:%M%p\"\n",
    "\"September 18, 2017, 22:19:55\" -> \"%B %d, %Y, %H:%M:%S\"\n",
    "\"Sun,05/12/99,12:30PM\" -> \"%a,%d/%m/%y,%I:%M%p\"\n",
    "\"Mon, 21 March, 2015\" -> \"%a, %d %B, %Y\"\n",
    "\"2018-03-12T10:12:45Z\" -> \"%Y-%m-%dT%H:%M:%SZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time, sleep\n",
    "print('sdfsd')\n",
    "for i in range(10):\n",
    "    sleep(.2)\n",
    "    print('\\r'+str(i),end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "id_of_tweet = \"1207211310456463360\"\n",
    "\n",
    "tweet = api.get_status(id_of_tweet, tweet_mode='extended')._json['full_text']\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2 = json.loads(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Merge dictionaries\n",
    "#             user_events.update(twitter_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsn = \"{\\\"user\\\": {\\\"id\\\": 1026860244, \\\"name\\\": \\\"Spaceman Spiff\\\", \\\"screen_name\\\": \\\"rajreddynyc\\\", \\\"location\\\": \\\"Queens, NY\\\", \\\"url\\\": null, \\\"protected\\\": false, \\\"verified\\\": false, \\\"followers_count\\\": 242, \\\"friends_count\\\": 54, \\\"listed_count\\\": 62, \\\"favourites_count\\\": 3108, \\\"statuses_count\\\": 193145, \\\"created_at\\\": \\\"Fri Dec 21 18:05:02 +0000 2012\\\"}, \\\"tweet\\\": {\\\"created_at\\\": \\\"Sat Dec 21 19:42:56 +0000 2019\\\", \\\"id\\\": 1208472941035438080, \\\"text\\\": \\\"@shambhav15 @Sanjay_Dixit the poem was like distributing nazi pamphlets in Israel.\\\", \\\"geo\\\": null, \\\"coordinates\\\": null, \\\"place\\\": null, \\\"quote_count\\\": 0, \\\"reply_count\\\": 0, \\\"retweet_count\\\": 0, \\\"favorite_count\\\": 0}}\"\n",
    "jsn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json.loads(jsn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split, from_json\n",
    "from time import time, sleep\n",
    "\n",
    "from pyspark.sql.types import StructType, StringType, MapType, StructField\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_keys = ['created_at', 'id', 'text', 'geo', 'coordinates', 'place',\n",
    "                          \"quote_count\", \"reply_count\", \"retweet_count\", \"favorite_count\" ]\n",
    "\n",
    "user_keys = ['id', 'name', 'screen_name','created_at', 'location', 'url',\n",
    "             'protected', 'verified', 'followers_count', 'friends_count',\n",
    "             'listed_count', 'favourites_count', 'statuses_count', 'withheld_in_countries']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get week number (week of the year)\n",
    "date_time_obj = datetime.strptime(\"2017-12-02 03:04:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "date_time_obj.strftime(\"%V\")\n",
    "\n",
    "#datetime.today().strftime(\"%V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "fs = pa.hdfs.connect(\n",
    "    host='localhost', \n",
    "    port=8020, \n",
    "    user='hdfs', \n",
    "    kerb_ticket=None, \n",
    "    driver='libhdfs', \n",
    "    extra_conf=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usrs = \"/user/hive/warehouse/twitter.db/users_staging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(usrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = fs.ls(usrs)[4]\n",
    "print(d)\n",
    "fs.mv(d, d.lower())\n",
    "\n",
    "fs.rename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I had folders names like '/user/hive/warehouse/twitter.db/users_staging/created_Ym=200703'\n",
    "# but hive creates columns names in lower case, so it couldn't find  created_ym=200703\n",
    "# had to rename them in bulk\n",
    "\n",
    "for d in fs.ls(usrs):\n",
    "    print(d)\n",
    "    try:\n",
    "        fs.mv(d, d.lower())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 9870\n",
    "hive_port = 10000\n",
    "hive_username = 'hdfs'\n",
    "hive_password = 'naya'\n",
    "hive_database = 'twitter'\n",
    "hive_mode = 'CUSTOM'\n",
    "\n",
    "hive_cnx = hive.Connection(\n",
    "  host = hdfs_host, \n",
    "  port = hive_port, \n",
    "  username = hive_username,\n",
    "  password = hive_password,\n",
    "  auth = hive_mode,\n",
    "  database = hive_database )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = f\"select user_id from tweets \"\n",
    "res = ''\n",
    "with hive_cnx.cursor() as cursor:\n",
    "    cursor.execute(q)\n",
    "    res = cursor.fetchall()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [r[0] for r in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(A, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A1 = np.array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = A1[A1<6e16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(B, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(A1,bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(B,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = int(4523454056235238756239857623587263450238672309276028762034123125345245234562346363452345234523452345.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to print with colored text\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    \n",
    "print(bcolors.OKGREEN + \"Warning: No active frommets remain. Continue?\" + bcolors.ENDC + \"kapisch?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics on Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 9870\n",
    "hive_port = 10000\n",
    "hive_username = 'hdfs'\n",
    "hive_password = 'naya'\n",
    "hive_database = 'twitter'\n",
    "hive_mode = 'CUSTOM'\n",
    "\n",
    "hive_cnx = hive.Connection(\n",
    "  host = hdfs_host, \n",
    "  port = hive_port, \n",
    "  username = hive_username,\n",
    "  password = hive_password,\n",
    "  auth = hive_mode,\n",
    "  database = hive_database )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = f\"select id, created_at from users \"\n",
    "res = ''\n",
    "with hive_cnx.cursor() as cursor:\n",
    "    cursor.execute(q)\n",
    "    res = cursor.fetchall()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [r[0] for r in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "A = pd.DataFrame(res, columns=('id','ts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "def timeStamp_fmt_convert(date):        \n",
    "    # Convert twitter datetime format such as \n",
    "    # \"Sat Jan 4 11:39:13 +0500 2019\"\n",
    "    # to partitioning-compatible format and shift it to utc, so: \n",
    "    # \"2017-12-02 03:04:00\"\n",
    "    date_time_obj = datetime.strptime(date, \"%a %b %d %H:%M:%S %z %Y\")\n",
    "    date_time_obj.astimezone(tz=timezone.utc)\n",
    "    return date_time_obj.timestamp()#.strftime(\"%Y-%m-%d %H:%M:%S\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = [timeStamp_fmt_convert(a) for a in A['ts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = ts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A['ts'] = ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "x = A['ts']\n",
    "y = A['id']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m/%d/%Y'))\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator())\n",
    "plt.plot(x,y)\n",
    "plt.gcf().autofmt_xdate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(A['ts'],A['id'], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = A[A['id']>2e17]\n",
    "\n",
    "len(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(B['ts'],B['id'], '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(A['ts'],bins = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "%matplotlib notebook\n",
    "\n",
    "from itertools import count\n",
    "import numpy as np\n",
    "\n",
    "n = 1000\n",
    "\n",
    "tweets1 = list(range(n))\n",
    "tweets2 = np.random.randn(n)\n",
    "tweets3 = np.random.randn(n)\n",
    "\n",
    "t = pd.DataFrame(list(zip(tweets1, tweets2, tweets3)),  columns = [\"a\",\"b\",\"c\"] )\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "x = []\n",
    "y1 = []\n",
    "y2 = []\n",
    "\n",
    "index = count()\n",
    "\n",
    "plt.plot([], [], label='Channel 1')\n",
    "plt.plot([], [], label='Channel 2')\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    data = t[:i]\n",
    "    x = data['a']\n",
    "    y1 = data['b']\n",
    "    y2 = data['c']\n",
    "\n",
    "    plt.cla()\n",
    "\n",
    "    plt.scatter(y1,x, label='Channel 1')\n",
    "    plt.scatter(y2,x, label='Channel 2')\n",
    "\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.xlim = [-5,5]\n",
    "    plt.ylim = [0,i+1]\n",
    "\n",
    "ani = FuncAnimation(plt.gcf(), animate, frames=range(1,n), interval=100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_temp():\n",
    "    return np.random.rand(1)[0]\n",
    "read_temp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import tmp102\n",
    "\n",
    "# Parameters\n",
    "x_len = 200         # Number of points to display\n",
    "y_range = [-5,5]  # Range of possible Y values to display\n",
    "\n",
    "# Create figure for plotting\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "xs = list(range(0, 200))\n",
    "ys = [0] * x_len\n",
    "ax.set_ylim(y_range)\n",
    "\n",
    "\n",
    "# Initialize communication with TMP102\n",
    "#tmp102.init()\n",
    "\n",
    "# Create a blank line. We will update the line in animate\n",
    "line, = ax.plot(xs, ys)\n",
    "\n",
    "# Add labels\n",
    "plt.title('TMP102 Temperature over Time')\n",
    "plt.xlabel('Samples')\n",
    "plt.ylabel('Temperature (deg C)')\n",
    "\n",
    "# This function is called periodically from FuncAnimation\n",
    "def animate(i, ys):\n",
    "\n",
    "    # Read temperature (Celsius) from TMP102\n",
    "    temp_c = round(read_temp(), 2)\n",
    "\n",
    "    # Add y to list\n",
    "    ys.append(temp_c)\n",
    "\n",
    "    # Limit y list to set number of items\n",
    "    ys = ys[-x_len:]\n",
    "\n",
    "    # Update line with new Y values\n",
    "    line.set_ydata(ys)\n",
    "\n",
    "    return line,\n",
    "\n",
    "# Set up plot to call animate() function periodically\n",
    "ani = animation.FuncAnimation(fig,\n",
    "    animate,\n",
    "    fargs=(ys,),\n",
    "    interval=50,\n",
    "    blit=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "from matplotlib import dates as mdates\n",
    "from datetime import datetime\n",
    "Data=['16:24:00',\n",
    "     '17:48:00',\n",
    "     '16:10:00',\n",
    "     '16:46:00',\n",
    "     '17:13:00',\n",
    "     '15:31:00',\n",
    "     '16:23:00',\n",
    "     '16:53:00',\n",
    "     '16:28:00',\n",
    "     '16:33:00',\n",
    "     '17:38:00',\n",
    "     '17:08:00',\n",
    "     '16:29:00',\n",
    "     '16:25:00',\n",
    "     '16:17:00',\n",
    "     '17:38:00',\n",
    "     '16:29:00']\n",
    "#convert strings into datetime objects\n",
    "conv_time = [datetime.strptime(i, \"%H:%M:%S\") for i in Data]\n",
    "#define bin number\n",
    "bin_nr = 7\n",
    "fig, ax = plt.subplots(1,1)\n",
    "#create histogram, get bin position for label\n",
    "_counts, bins, _patches = ax.hist(conv_time, bins = bin_nr)\n",
    "#set xticks at bin edges\n",
    "plt.xticks(bins)\n",
    "#reformat bin label into format hour:minute\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%H:%M\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import threading, queue\n",
    "\n",
    "class WorkerThread(threading.Thread):\n",
    "    \"\"\" A worker thread that takes directory names from a queue, finds all\n",
    "        files in them recursively and reports the result.\n",
    "\n",
    "        Input is done by placing directory names (as strings) into the\n",
    "        Queue passed in dir_q.\n",
    "\n",
    "        Output is done by placing tuples into the Queue passed in result_q.\n",
    "        Each tuple is (thread name, dirname, [list of files]).\n",
    "\n",
    "        Ask the thread to stop by calling its join() method.\n",
    "    \"\"\"\n",
    "    def __init__(self, dir_q, result_q):\n",
    "        super(WorkerThread, self).__init__()\n",
    "        self.dir_q = dir_q\n",
    "        self.result_q = result_q\n",
    "        self.stoprequest = threading.Event()\n",
    "        self.data =0\n",
    "        print(self.name + \" initialized\")\n",
    "    def run(self):\n",
    "        # As long as we weren't asked to stop, try to take new tasks from the\n",
    "        # queue. The tasks are taken with a blocking 'get', so no CPU\n",
    "        # cycles are wasted while waiting.\n",
    "        # Also, 'get' is given a timeout, so stoprequest is always checked,\n",
    "        # even if there's nothing in the queue.\n",
    "        while not self.stoprequest.isSet():\n",
    "            try:\n",
    "                num = self.dir_q.get(True, 0.05)\n",
    "                print(self.name + \" counts to \" + str(num))\n",
    "                numlist = list(self._count(num))\n",
    "                self.result_q.put((self.name, num, numlist))\n",
    "            except queue.Empty:\n",
    "                continue\n",
    "    def start(self):\n",
    "        print(self.name + \" started\")\n",
    "        super(WorkerThread, self).start()\n",
    "                \n",
    "    def join(self, timeout=None):\n",
    "        self.stoprequest.set()\n",
    "        print(self.name + \" X_X\")\n",
    "        super(WorkerThread, self).join(timeout)\n",
    "\n",
    "    def _count(self, to):\n",
    "        for i in range(to):\n",
    "            time.sleep(1)\n",
    "            #print(\"\\r\" + str(i), end = '')\n",
    "            self.data = i\n",
    "            yield i\n",
    "    def ask(self):\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-6 initialized\n"
     ]
    }
   ],
   "source": [
    "# def main(args):\n",
    "# Create a single input and a single output queue for all threads.\n",
    "dir_q = queue.Queue()\n",
    "result_q = queue.Queue()\n",
    "\n",
    "# Create the \"thread pool\"\n",
    "pool = [WorkerThread(dir_q=dir_q, result_q=result_q) for i in range(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-6 started\n"
     ]
    }
   ],
   "source": [
    "# Start all threads\n",
    "for thread in pool:\n",
    "    thread.start()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool[0].run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-6 counts to 5\n",
      "Thread-6 counts to 6\n",
      "Thread-6 counts to 7\n",
      "Thread-6 counts to 8\n",
      "Thread-6 counts to 9\n"
     ]
    }
   ],
   "source": [
    "pool[0].ask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-df5ffcdda271>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# wake event thread (message content is ignored)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    393\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[1;32m    394\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    for thread in pool:\n",
    "        a= thread.ask()\n",
    "        print(\"\\r\" + str(a), end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigned 6 nums to workers\n",
      "Thread-6 counts to 60\n"
     ]
    }
   ],
   "source": [
    "nums = [60, 5, 6, 7, 8, 9]\n",
    "# Give the workers some work to do\n",
    "work_count = 0\n",
    "for n in nums:\n",
    "    work_count += 1\n",
    "    dir_q.put(n)\n",
    "\n",
    "print(f'Assigned {work_count} nums to workers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From thread Thread-6: 4 files found in dir 4\n",
      "From thread Thread-8: 5 files found in dir 5\n",
      "From thread Thread-7: 6 files found in dir 6\n",
      "From thread Thread-9: 7 files found in dir 7\n",
      "From thread Thread-5: 8 files found in dir 8\n",
      "From thread Thread-4: 9 files found in dir 9\n"
     ]
    }
   ],
   "source": [
    "# Now get all the results\n",
    "while work_count > 0:\n",
    "    # Blocking 'get' from a Queue.\n",
    "    result = result_q.get()\n",
    "    print('From thread %s: %s files found in dir %s' % (result[0], len(result[2]), result[1]))\n",
    "    work_count -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread-4 X_X\n",
      "Thread-5 X_X\n",
      "Thread-6 X_X\n",
      "Thread-7 X_X\n",
      "Thread-8 X_X\n",
      "Thread-9 X_X\n"
     ]
    }
   ],
   "source": [
    "# Ask threads to die and wait for them to do it\n",
    "for thread in pool:\n",
    "    thread.join()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     import sys\n",
    "#     main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineer Course Project\n",
    "by [Michael Berger](https://www.linkedin.com/in/michael-berger-e/ \"My LinkedIn\")\n",
    "\n",
    "\n",
    "## About\n",
    "This is my final project for Data Engineer Expert course at Naya College.  \n",
    "The goal of this project is to create a multi-purpose platform that acquires data from the Twitter developers API (in real-time), based on certain keywords of interest. Once the data is collected, the platform wil perform data filtering, cleansing and enrichment to perform analytical operations for the organizational and business purposes and to transform raw data into knowledge.\n",
    "![alt text](Architecture.png \"Platform Design\")\n",
    "\n",
    "## Requirements\n",
    "To run this platform you will need:  \n",
    "- A modern machine with at least 16 GB RAM and 100 GB Storage\n",
    "- A cluster of these, in case you want to use the platform at any significant scale\n",
    "- A Hadoop distribution ([Cloudera QuickStart Virtual Machine](https://www.cloudera.com/downloads/quickstart_vms/5-13.html)  will be sufficient)\n",
    "\n",
    "## Installation\n",
    "- Set up your Virtual Machine\n",
    "- Git clone the repo to your home dir\n",
    "- Run file configure_environment.sh\n",
    "- Edit configuration in config.py according to your liking / working environment \n",
    "\n",
    "## Usage\n",
    "- Run notebook HDFS_DB_Init.ipynb from start to end. This will create HDFS directories and HIVE tables  \n",
    "- Run notebook twitter_Producer.ipynb from start to end. It will start receiveing tweets from twitter API and sending them to Kafka topic.  \n",
    "- Run notebook Spark_Consumer.ipynb. It will start processing data in Spark, writing it to HDFS and refreshing HIVE tables. Do not run one-line-cells below the df statements. These are for monitoring and debugging only.  \n",
    "- Run notebook Dashboard.ipynb to see some graphs and distributions of data being collected\n",
    "\n",
    "\n",
    "\n",
    "The files HDFS_Consumer.ipynb, HIVE_Consumer.ipynb and Sandbox.ipynb are generally not needed. The may stay as examples of various code snippets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
