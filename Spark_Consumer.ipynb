{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "\n",
    "-find how to make output to 2 places : save json to hdfs and table to hive\n",
    "\n",
    "-how come sometimes there are 2 tweets in one output, like:\n",
    "<pre>\n",
    "+----+--------------------+  \n",
    "| key|               value|  \n",
    "+----+--------------------+  \n",
    "|null|\"@SenWarren Need ...|  \n",
    "|null|\"RT @TrinityResis...|  \n",
    "+----+--------------------+  \n",
    "</pre>\n",
    "-how to consume from a kafka topic everything there is up until now (cursor)  \n",
    "-how to cope with shit like  <pre>\\\\ud83c\\\\uddf5\\\\ud83c\\\\uddf8\\\\u2764\\\\</pre> instead of arabic chars https://stackoverflow.com/questions/18337407/saving-utf-8-texts-in-json-dumps-as-utf8-not-as-u-escape-sequence\n",
    "\n",
    "\n",
    "-how to dump data to HDFS in large parque files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys, os\n",
    "from time import time, sleep\n",
    "from itertools import cycle\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, split, from_json, flatten, udf\n",
    "\n",
    "from common_vars import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweet_keys))\n",
    "print(len(tweet_types))\n",
    "print(len(user_keys))\n",
    "print(len(user_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poll_continiously(Query, period = .35, attrname = 'status'):\n",
    "    chars = \"⠁⠂⠄⡀⢀⠠⠐⠈\"\n",
    "    #chars = '←↖↑↗→↘↓↙'\n",
    "    #chars = 'wingardium_leviosa '\n",
    "    #chars = 'I solemnly swear that I am up to no good'.replace(' ','_')+' '\n",
    "    spinner = cycle(chars)\n",
    "    while True:\n",
    "        try:\n",
    "            sleep(period)\n",
    "            val = getattr(Query, attrname)\n",
    "            print('\\r'+next(spinner)+repr(val)[:100]+(' '*100) ,end = '')\n",
    "        except KeyboardInterrupt:\n",
    "            print('\\rSTOPPED polling (KeyboardInterrupt)'+(' '*100) ,end = '')\n",
    "            break    \n",
    "        except Exception as e :\n",
    "            errors.append([year,e])\n",
    "            print(\"oops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"StructuredTwitterJsonArchive\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assembleStructType(ST,keys,types):\n",
    "    ST = ST.add(keys[0],types[0]())\n",
    "    if len(keys)>1:\n",
    "        return assembleStructType(ST,keys[1:],types[1:])\n",
    "    else:\n",
    "        return ST\n",
    "\n",
    "test = assembleStructType(StructType(), tweet_keys, tweet_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value schema: { \"user\": \"user_keys\", \"tweet\": \"tweet_keys\" }\n",
    "\n",
    "tweet_struct = assembleStructType(StructType(), tweet_keys, tweet_types)\n",
    "\n",
    "user_struct = assembleStructType(StructType(), user_keys, user_types)\n",
    "\n",
    "schema = StructType().add(\"users\", user_struct).add(\"tweets\", tweet_struct)\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", topic) \\\n",
    "  .option(\"failOnDataLoss\" , \"false\")\\\n",
    "  .load()\\\n",
    "  .select(col(\"key\").cast(\"string\"), from_json(col(\"value\").cast(\"string\"), schema).alias('value'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw JSON Data to archive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json to HDFS sink with partitioning\n",
    "\n",
    "targetJsonHDFS = df\\\n",
    "    .select(col('key'),col('value.*'))\\\n",
    "    .writeStream\\\n",
    "    .format(\"json\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .partitionBy(\"key\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_archive_path)\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_archive_checkpoint_path)\\\n",
    "    .start()\n",
    "#targetJsonHDFS.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(targetJsonHDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets to HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Sentiment(sent):\n",
    "    from textblob import TextBlob\n",
    "    return int(TextBlob(sent).sentiment.polarity*100)\n",
    "\n",
    "Sentiment_udf = udf(Sentiment, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def n_words(sent):\n",
    "    from textblob import TextBlob\n",
    "    return len(TextBlob(sent).words)\n",
    "n_words(\"hello how are you\")\n",
    "\n",
    "n_words_udf = udf(n_words, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump new tweets to HDFS as Parquet (small files):\n",
    "tweets2HIVE = df\\\n",
    "    .select(col('key'),col(f'value.tweets.*'))\\\n",
    "    .withColumn('Sentiment',Sentiment_udf('text'))\\\n",
    "    .withColumn('n_words',n_words_udf('text'))\\\n",
    "    .where(\"n_words > 10\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .partitionBy(\"key\")\\\n",
    "    .trigger(processingTime=\"1 minutes\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_hive_tweets)\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_hive_tweets + \"/sparkcheckpoint\" )\\\n",
    "    .start()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(tweets2HIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2HIVE.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2HIVE.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2HIVE.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users to HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump new users to HDFS as Parquet (small files): \n",
    "users2HIVE = df\\\n",
    "    .select(col('key').alias('last_tweet_at'), col('value.users.*'))\\\n",
    "    .dropDuplicates(subset=['id'])\\\n",
    "    .writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .partitionBy(partitionCol)\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_hive_users_staging)\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_hive_users_staging + \"/sparkcheckpoint\" )\\\n",
    "    .start()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(users2HIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2HIVE.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2HIVE.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2HIVE.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIVE refresher job for tweets and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_hive_table(df, epoch_id):\n",
    "    from pyhive import hive\n",
    "    hive_cnx = hive.Connection(\n",
    "        host = 'localhost', \n",
    "        port = 10000, \n",
    "        username = 'hdfs',\n",
    "        password = 'naya',\n",
    "        auth = 'CUSTOM',\n",
    "        database='twitter')\n",
    "    with hive_cnx.cursor() as cursor:    \n",
    "        cursor.execute(\" MSCK REPAIR TABLE tweets \")      \n",
    "        cursor.execute(\" MSCK REPAIR TABLE users \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVErefresher = df\\\n",
    "                .writeStream\\\n",
    "                .foreachBatch(refresh_hive_table)\\\n",
    "                .trigger( processingTime='1 minute')\\\n",
    "                .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(HIVErefresher, period=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVErefresher.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVErefresher.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make users unique by 4-step aggregation query (does not work)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "possible solutions to \"failed to move file\" problem \n",
    "\n",
    "1)\n",
    "    add the property to hdfs-site.xml\n",
    "(/etc/hive/conf/hive-site.xml )\n",
    "    <property>\n",
    "        <name>fs.hdfs.impl.disable.cache</name>\n",
    "        <value>true</value>\n",
    "    </property> \n",
    "from thread: https://stackoverflow.com/questions/48592337/hive-hadoop-intermittent-failure-unable-to-move-source-to-destination\n",
    "\n",
    "\n",
    "2) make users table internal also, so that drop will erase everything including folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "\n",
    "def assemble_queries(year):\n",
    "    queries = []\n",
    "\n",
    "    queries.append(\"SET hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "    queries.append(\"SET hive.exec.dynamic.partition = true;\")\n",
    "    queries.append(\"SET hive.mapred.mode = nonstrict;\")\n",
    "    queries.append(\"SET hive.optimize.sort.dynamic.partition=false\")\n",
    "    queries.append(\"SET spark.sql.hive.convertMetastoreParquet=false\")\n",
    "    #queries.append(\"SET hive.exec.stagingdir=.hive-staging;\")\n",
    "    queries.append(\"MSCK REPAIR TABLE users_staging\")\n",
    "\n",
    "    cols = ', '.join(colNames)\n",
    "\n",
    "    queries.append(f\"insert into users_staging partition({partitionCol}={year}) select {cols} from users where {partitionCol}={year}\")\n",
    "\n",
    "    #queries.append(f\"alter table users drop partition ({partitionCol}={year})\")\n",
    "\n",
    "    #This is probably the right query for the 'merge' of partitions\n",
    "    q42 = f\"\"\"\n",
    "    SELECT last_tweet_at, id, name, screen_name, created_at, `location` , url, protected, verified, followers_count, friends_count, \n",
    "           listed_count, favourites_count, statuses_count, withheld_in_countries\n",
    "    FROM (\n",
    "        SELECT last_tweet_at, id, name, screen_name, created_at, `location` , url, protected, verified, followers_count, friends_count, \n",
    "           listed_count, favourites_count, statuses_count, withheld_in_countries,\n",
    "           rank() over (partition by id order by last_tweet_at desc) as last_rank\n",
    "        from users_staging\n",
    "        where {partitionCol} ={year}\n",
    "    ) ranked_users\n",
    "    WHERE ranked_users.last_rank = 1\n",
    "    \"\"\"\n",
    "\n",
    "    queries.append(f\"insert overwrite table users partition({partitionCol}={year}) {q42}\")\n",
    "\n",
    "    queries.append(f\"alter table users_staging drop partition ({partitionCol}={year})\")\n",
    "    return queries\n",
    "\n",
    "assemble_queries('201703')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q = \"select distinct created_ym from users_staging order by created_ym desc\"\n",
    "queries = [\"MSCK REPAIR TABLE users_staging\"]\n",
    "queries.append(\"select created_ym, count(created_ym) from users_staging group by created_ym\")\n",
    "res = run_queries(queries)\n",
    "\n",
    "years = [r[0] for r in res]\n",
    "years = cycle(years)\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:    \n",
    "    year = next(years)\n",
    "    print('|' + year , end = '')\n",
    "    try:\n",
    "        queries = assemble_queries(year)\n",
    "        run_queries(queries)\n",
    "    except KeyboardInterrupt:\n",
    "        print('KeyboardInterrupt')\n",
    "        break    \n",
    "    except Exception as e :\n",
    "        errors.append([year,e])\n",
    "        print(\"oops\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '201703'\n",
    "\n",
    "queries = []\n",
    "\n",
    "queries.append(\"SET hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "queries.append(\"SET hive.exec.dynamic.partition = true;\")\n",
    "queries.append(\"SET hive.mapred.mode = nonstrict;\")\n",
    "queries.append(\"SET hive.optimize.sort.dynamic.partition=false\")\n",
    "queries.append(\"SET spark.sql.hive.convertMetastoreParquet=false\")\n",
    "queries.append(\"SET hive.exec.stagingdir=.hive-staging;\")\n",
    "queries.append(\"MSCK REPAIR TABLE users_staging\")\n",
    "\n",
    "cols = ', '.join(colNames)\n",
    "\n",
    "queries.append(f\"insert into users_staging partition({partitionCol}) select {cols} from users\")\n",
    "\n",
    "#queries.append(f\"alter table users drop partition ({partitionCol}={year})\")\n",
    "\n",
    "#This is probably the right query for the 'merge' of partitions\n",
    "q42 = f\"\"\"\n",
    "SELECT last_tweet_at, id, name, screen_name, created_at, `location` , url, protected, verified, followers_count, friends_count, \n",
    "       listed_count, favourites_count, statuses_count, withheld_in_countries, created_ym \n",
    "FROM (\n",
    "    SELECT last_tweet_at, id, name, screen_name, created_at, `location` , url, protected, verified, followers_count, friends_count, \n",
    "       listed_count, favourites_count, statuses_count, withheld_in_countries, created_ym, \n",
    "       rank() over (partition by id order by last_tweet_at desc) as last_rank\n",
    "    from users_staging        \n",
    ") ranked_users\n",
    "WHERE ranked_users.last_rank = 1\n",
    "\"\"\"\n",
    "\n",
    "queries.append(f\"insert overwrite table users partition({partitionCol}) {q42}\")\n",
    "\n",
    "#queries.append(f\"alter table users_staging drop partition ({partitionCol}={year})\")\n",
    "\n",
    "print(queries[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark.SQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = df \\\n",
    "    .select(col(\"value.tweet.*\"))\\\n",
    "    .writeStream \\\n",
    "    .queryName(\"aggregate2\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = spark.sql(\"select * from aggregate2\") # interactively query in-memory table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b  = a.write.parquet(\"hdfs://localhost:8020/tmp/project/tmp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.show(10,200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/tmp/project/hive/key=2019-12-25_18-15/part-00000-c5543d3a-7e1d-467d-a0d7-9b8893fe9bb4.c000.snappy.parquet\"\n",
    "c = spark.read.parquet('hdfs://localhost:8020'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from aggregate2\").show(100,1100) # interactively query in-memory table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox \n",
    "below are cells that I played with to test portions of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "options={\"timestampFormat\": \"%a %b %d %H:%M:%S %z %Y\"} #\"yyyy-MM-dd HH:mm:ss\"}\n",
    "options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " schema = StructType().add(\"value\", StructType().add(\"user\", StringType())\n",
    "                                            .add(\"tweet\", StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " schema = StructType().add(\"value\", StructType().add(\"value\", StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, MapType, StructField\n",
    "\n",
    "\n",
    "schema = StructType().add('created_at', StringType(), False).add('id_str', StringType(), False)\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"value\", StringType(), True), True),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(sampleFilePath, schema, multiLine=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = spark.read.option(\"multiLine\", True).json(sampleFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsondf = spark.read.json(Seq(jsonstr).toDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = schema.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleFilePath = \"/home/naya/DataEngineerProject/part-00000-7ad71167-a959-4478-929a-6ab7607844d4.c000.json\"\n",
    "sampleFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(sampleFilePath) as f:\n",
    "    txt = f.read()\n",
    "    \n",
    "(txt.__repr__())\n",
    "\n",
    "json.loads(txt.replace('\\\\\"','\\\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleFilePath = \"hdfs://localhost:8020\" +\\\n",
    "                \"/tmp/project/archive/key=2019-12-22_10-18/part-00000-7ad71167-a959-4478-929a-6ab7607844d4.c000.json\"\n",
    "sampleFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sf = sqlContext.read.json(sampleFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json to HDFS sink with trigger\n",
    "\n",
    "targetJsonHDFS = df\\\n",
    "    .writeStream\\\n",
    "    .format(\"json\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_archive_path)\\\n",
    "    .trigger(processingTime=\"5 seconds\")\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_archive_checkpoint_path)\\\n",
    "    .start()\n",
    "targetJsonHDFS.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet to HDFS sink example with trigger\n",
    "\n",
    "targetParquetHDFS = df\\\n",
    "    .writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020/tmp/project\")\\\n",
    "    .trigger(processingTime=\"5 seconds\")\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020/tmp/sparkcheckpoint/\")\\\n",
    "    .start()\n",
    "targetParquetHDFS.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetParquetHDFS.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write parquet files to local linux dir\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\")       \\\n",
    "    .option(\"path\", \"/tmp/project\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/sparkcheckpoint/\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write parquet files to local linux dir\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\")       \\\n",
    "    .option(\"path\", \"/tmp/project\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/sparkcheckpoint/\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output to console\n",
    "# query = df \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = df.select(\n",
    "#    explode(\n",
    "#        split(df.value, \" \")\n",
    "#    ).alias(\"word\")\n",
    "# )\n",
    "\n",
    "# Generate running word count\n",
    "#wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = wordCounts \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "# query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
