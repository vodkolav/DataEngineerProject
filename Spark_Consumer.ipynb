{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys, os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, split, from_json, flatten, udf\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweet_keys))\n",
    "print(len(tweet_types))\n",
    "print(len(user_keys))\n",
    "print(len(user_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"StructuredTwitterJsonArchive\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assembleStructType(ST,keys,types):\n",
    "    ST = ST.add(keys[0],types[0]())\n",
    "    if len(keys)>1:\n",
    "        return assembleStructType(ST,keys[1:],types[1:])\n",
    "    else:\n",
    "        return ST\n",
    "\n",
    "test = assembleStructType(StructType(), tweet_keys, tweet_types)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value schema: { \"user\": \"user_keys\", \"tweet\": \"tweet_keys\" }\n",
    "\n",
    "tweet_struct = assembleStructType(StructType(), tweet_keys, tweet_types)\n",
    "\n",
    "user_struct = assembleStructType(StructType(), user_keys, user_types)\n",
    "\n",
    "schema = StructType().add(\"users\", user_struct).add(\"tweets\", tweet_struct)\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", topic) \\\n",
    "  .option(\"failOnDataLoss\" , \"false\")\\\n",
    "  .load()\\\n",
    "  .select(col(\"key\").cast(\"string\"), from_json(col(\"value\").cast(\"string\"), schema).alias('value'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw JSON Data to archive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json to HDFS sink with partitioning\n",
    "\n",
    "targetJsonHDFS = df\\\n",
    "    .select(col('key'),col('value.*'))\\\n",
    "    .writeStream\\\n",
    "    .format(\"json\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .partitionBy(\"key\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_archive_path)\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_archive_path + \"/sparkcheckpoint\")\\\n",
    "    .start()\n",
    "#targetJsonHDFS.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(targetJsonHDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets to HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Sentiment(sent):\n",
    "    from textblob import TextBlob\n",
    "    return int(TextBlob(sent).sentiment.polarity*100)\n",
    "\n",
    "Sentiment_udf = udf(Sentiment, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def n_words(sent):\n",
    "    from textblob import TextBlob\n",
    "    return len(TextBlob(sent).words)\n",
    "n_words(\"hello how are you\")\n",
    "\n",
    "n_words_udf = udf(n_words, IntegerType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump new tweets to HDFS as Parquet (small files):\n",
    "tweets2HIVE = df\\\n",
    "    .select(col('key'),col(f'value.tweets.*'))\\\n",
    "    .withColumn('Sentiment',Sentiment_udf('text'))\\\n",
    "    .withColumn('n_words',n_words_udf('text'))\\\n",
    "    .where(\"n_words > 10\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .partitionBy(\"key\")\\\n",
    "    .trigger(processingTime=\"1 minutes\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_hive_tweets)\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_hive_tweets + \"/sparkcheckpoint\" )\\\n",
    "    .start()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(tweets2HIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2HIVE.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2HIVE.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2HIVE.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users to HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump new users to HDFS as Parquet (small files): \n",
    "users2HIVE = df\\\n",
    "    .select(col('key').alias('last_tweet_at'), col('value.users.*'))\\\n",
    "    .dropDuplicates(subset=['id'])\\\n",
    "    .writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .partitionBy(partitionCol)\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_hive_users_staging)\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_hive_users_staging + \"/sparkcheckpoint\" )\\\n",
    "    .start()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(users2HIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2HIVE.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2HIVE.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2HIVE.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HIVE refresher job for tweets and users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_hive_table(df, epoch_id):\n",
    "    from pyhive import hive\n",
    "    hive_cnx = hive.Connection(\n",
    "        host = 'localhost', \n",
    "        port = 10000, \n",
    "        username = 'hdfs',\n",
    "        password = 'naya',\n",
    "        auth = 'CUSTOM',\n",
    "        database=hive_database)\n",
    "    with hive_cnx.cursor() as cursor:    \n",
    "        cursor.execute(\" MSCK REPAIR TABLE tweets \")      \n",
    "        cursor.execute(\" MSCK REPAIR TABLE users \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVErefresher = df\\\n",
    "                .writeStream\\\n",
    "                .foreachBatch(refresh_hive_table)\\\n",
    "                .trigger( processingTime='1 minute')\\\n",
    "                .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(HIVErefresher, period=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVErefresher.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVErefresher.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make users unique by 4-step aggregation query (does not work)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "possible solutions to \"failed to move file\" problem \n",
    "\n",
    "1)\n",
    "    add the property to hdfs-site.xml\n",
    "(/etc/hive/conf/hive-site.xml )\n",
    "    <property>\n",
    "        <name>fs.hdfs.impl.disable.cache</name>\n",
    "        <value>true</value>\n",
    "    </property> \n",
    "from thread: https://stackoverflow.com/questions/48592337/hive-hadoop-intermittent-failure-unable-to-move-source-to-destination\n",
    "\n",
    "\n",
    "2) make users table internal also, so that drop will erase everything including folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "\n",
    "def assemble_queries(year):\n",
    "    queries = []\n",
    "\n",
    "    queries.append(\"SET hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "    queries.append(\"SET hive.exec.dynamic.partition = true;\")\n",
    "    queries.append(\"SET hive.mapred.mode = nonstrict;\")\n",
    "    queries.append(\"SET hive.optimize.sort.dynamic.partition=false\")\n",
    "    queries.append(\"SET spark.sql.hive.convertMetastoreParquet=false\")\n",
    "    #queries.append(\"SET hive.exec.stagingdir=.hive-staging;\")\n",
    "    queries.append(\"MSCK REPAIR TABLE users_staging\")\n",
    "\n",
    "    cols = ', '.join(colNames)\n",
    "\n",
    "    queries.append(f\"insert into users_staging partition({partitionCol}={year}) select {cols} from users where {partitionCol}={year}\")\n",
    "\n",
    "    #queries.append(f\"alter table users drop partition ({partitionCol}={year})\")\n",
    "\n",
    "    #This is probably the right query for the 'merge' of partitions\n",
    "    q42 = f\"\"\"\n",
    "    SELECT last_tweet_at, id, name, screen_name, created_at, `location` , url, protected, verified, followers_count, friends_count, \n",
    "           listed_count, favourites_count, statuses_count, withheld_in_countries\n",
    "    FROM (\n",
    "        SELECT last_tweet_at, id, name, screen_name, created_at, `location` , url, protected, verified, followers_count, friends_count, \n",
    "           listed_count, favourites_count, statuses_count, withheld_in_countries,\n",
    "           rank() over (partition by id order by last_tweet_at desc) as last_rank\n",
    "        from users_staging\n",
    "        where {partitionCol} ={year}\n",
    "    ) ranked_users\n",
    "    WHERE ranked_users.last_rank = 1\n",
    "    \"\"\"\n",
    "\n",
    "    queries.append(f\"insert overwrite table users partition({partitionCol}={year}) {q42}\")\n",
    "\n",
    "    queries.append(f\"alter table users_staging drop partition ({partitionCol}={year})\")\n",
    "    return queries\n",
    "\n",
    "assemble_queries('201703')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q = \"select distinct created_ym from users_staging order by created_ym desc\"\n",
    "queries = [\"MSCK REPAIR TABLE users_staging\"]\n",
    "queries.append(\"select created_ym, count(created_ym) from users_staging group by created_ym\")\n",
    "res = run_queries(queries)\n",
    "\n",
    "years = [r[0] for r in res]\n",
    "years = cycle(years)\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:    \n",
    "#     year = next(years)\n",
    "#     print('|' + year , end = '')\n",
    "#     try:\n",
    "#         queries = assemble_queries(year)\n",
    "#         run_queries(queries)\n",
    "#     except KeyboardInterrupt:\n",
    "#         print('KeyboardInterrupt')\n",
    "#         break    \n",
    "#     except Exception as e :\n",
    "#         errors.append([year,e])\n",
    "#         print(\"oops\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '201703'\n",
    "\n",
    "queries = []\n",
    "\n",
    "queries.append(\"SET hive.exec.dynamic.partition.mode=nonstrict\")\n",
    "queries.append(\"SET hive.exec.dynamic.partition = true;\")\n",
    "queries.append(\"SET hive.mapred.mode = nonstrict;\")\n",
    "queries.append(\"SET hive.optimize.sort.dynamic.partition=false\")\n",
    "queries.append(\"SET spark.sql.hive.convertMetastoreParquet=false\")\n",
    "queries.append(\"SET hive.exec.stagingdir=.hive-staging;\")\n",
    "queries.append(\"MSCK REPAIR TABLE users_staging\")\n",
    "\n",
    "cols = ', '.join(colNames)\n",
    "\n",
    "queries.append(f\"insert into users_staging partition({partitionCol}) select {cols} from users\")\n",
    "\n",
    "#queries.append(f\"alter table users drop partition ({partitionCol}={year})\")\n",
    "\n",
    "#This is probably the right query for the 'merge' of partitions\n",
    "q42 = f\"\"\"\n",
    "SELECT last_tweet_at, id, name, screen_name, created_at, `location` , url, protected, verified, followers_count, friends_count, \n",
    "       listed_count, favourites_count, statuses_count, withheld_in_countries, created_ym \n",
    "FROM (\n",
    "    SELECT last_tweet_at, id, name, screen_name, created_at, `location` , url, protected, verified, followers_count, friends_count, \n",
    "       listed_count, favourites_count, statuses_count, withheld_in_countries, created_ym, \n",
    "       rank() over (partition by id order by last_tweet_at desc) as last_rank\n",
    "    from users_staging        \n",
    ") ranked_users\n",
    "WHERE ranked_users.last_rank = 1\n",
    "\"\"\"\n",
    "\n",
    "queries.append(f\"insert overwrite table users partition({partitionCol}) {q42}\")\n",
    "\n",
    "#queries.append(f\"alter table users_staging drop partition ({partitionCol}={year})\")\n",
    "\n",
    "print(queries[-1])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
