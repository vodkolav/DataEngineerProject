{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Consumer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "\n",
    "-find how to make output to 2 places : save json to hdfs and table to hive\n",
    "\n",
    "-how come sometimes there are 2 tweets in one output, like:\n",
    "<pre>\n",
    "+----+--------------------+  \n",
    "| key|               value|  \n",
    "+----+--------------------+  \n",
    "|null|\"@SenWarren Need ...|  \n",
    "|null|\"RT @TrinityResis...|  \n",
    "+----+--------------------+  \n",
    "</pre>\n",
    "-how to consume from a kafka topic everything there is up until now (cursor)  \n",
    "-how to cope with shit like  <pre>\\\\ud83c\\\\uddf5\\\\ud83c\\\\uddf8\\\\u2764\\\\</pre> instead of arabic chars https://stackoverflow.com/questions/18337407/saving-utf-8-texts-in-json-dumps-as-utf8-not-as-u-escape-sequence\n",
    "\n",
    "\n",
    "-how to dump data to HDFS in large parque files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pyarrow as pa\n",
    "import sys, os\n",
    "from time import time, sleep\n",
    "from itertools import cycle\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col, split, from_json, flatten\n",
    "from pyspark.sql.types import StructType, StringType, MapType, StructField,\\\n",
    "                              BooleanType, DateType, NumericType, IntegerType,\\\n",
    "                              LongType, TimestampType, FloatType, ArrayType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.1 pyspark-shell' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE WITH CAUTION\n",
    "\n",
    "#DELETE and recreate the data and checkpoint directories in hdfs \n",
    "\n",
    "fs = pa.hdfs.connect(\n",
    "    host='localhost', \n",
    "    port=8020, \n",
    "    user='hdfs', \n",
    "    kerb_ticket=None, \n",
    "    driver='libhdfs', \n",
    "    extra_conf=None)\n",
    "\n",
    "def reset_dir(hdfs_path):\n",
    "    if fs.exists(hdfs_path):\n",
    "        fs.delete(hdfs_path,recursive=True)\n",
    "        print(\"directory deleted: \" + hdfs_path)\n",
    "    else:\n",
    "        print(\"no such dir: \" + hdfs_path)\n",
    "\n",
    "    fs.mkdir(hdfs_path + '/sparkcheckpoint')    \n",
    "    print(\"directory created: \" + fs.ls(hdfs_path)[0])\n",
    "    fs.chmod(hdfs_path + '/sparkcheckpoint', 0o777)   \n",
    "    fs.chmod(hdfs_path, 0o777)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poll_continiously(Query, period = .35, attrname = 'status'):\n",
    "    #spinner = cycle('←↖↑↗→↘↓↙')\n",
    "    spinner = cycle('wingardium_leviosa ')\n",
    "    #spinner = cycle('I solemnly swear that I am up to no good'.replace(' ','_')+' ')\n",
    "    while True:\n",
    "        sleep(period)\n",
    "        val = getattr(Query, attrname)\n",
    "        print('\\r'+next(spinner)+repr(val)[:100]+(' '*100) ,end = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'TweeterArchive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_archive_path = '/tmp/project/archive'    \n",
    "hdfs_archive_checkpoint_path = hdfs_archive_path + '/sparkcheckpoint'\n",
    "\n",
    "hdfs_hive_warehouse = '/user/hive/warehouse'    \n",
    "\n",
    "hdfs_hive_staging = '/tmp/project/hive/tweets'\n",
    "hdfs_hive_staging_checkpoint = hdfs_hive_staging + '/sparkcheckpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(hdfs_archive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(hdfs_hive_staging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"StructuredTwitterJsonArchive\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_keys = ['id', 'text','created_at', 'geo', 'coordinates', 'place',\n",
    "              'quote_count', 'reply_count', 'retweet_count', 'favorite_count', 'user_id' ]\n",
    "\n",
    "tweet_types = [LongType, StringType, TimestampType, StringType, StringType, StringType, \n",
    "               IntegerType, IntegerType, IntegerType, IntegerType, LongType]\n",
    "\n",
    "# tweet_types  = [StringType]* 11 \n",
    "\n",
    "\n",
    "\n",
    "user_keys = ['id', 'name', 'screen_name','created_at', 'location', 'url',\n",
    "             'protected', 'verified', 'followers_count', 'friends_count',\n",
    "             'listed_count', 'favourites_count', 'statuses_count', 'withheld_in_countries', 'key']\n",
    "\n",
    "user_types = [LongType, StringType, StringType, TimestampType, StringType, StringType, \n",
    "              BooleanType, BooleanType, IntegerType, IntegerType, \n",
    "              IntegerType, IntegerType, IntegerType, StringType, LongType]\n",
    "\n",
    "# user_types = [StringType]* 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweet_keys))\n",
    "print(len(tweet_types))\n",
    "print(len(user_keys))\n",
    "print(len(user_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assembleStructType(ST,keys,types):\n",
    "    ST = ST.add(keys[0],types[0]())\n",
    "    if len(keys)>1:\n",
    "        return assembleStructType(ST,keys[1:],types[1:])\n",
    "    else:\n",
    "        return ST\n",
    "\n",
    "test = assembleStructType(StructType(), tweet_keys, tweet_types)\n",
    "test.fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value schema: { \"user\": \"user_keys\", \"tweet\": \"tweet_keys\" }\n",
    "\n",
    "tweet_struct = assembleStructType(StructType(), tweet_keys, tweet_types)\n",
    "\n",
    "user_struct = assembleStructType(StructType(), user_keys, user_types)\n",
    "\n",
    "schema = StructType().add(\"users\", user_struct).add(\"tweets\", tweet_struct)\n",
    "\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", topic) \\\n",
    "  .load()\\\n",
    "  .select(col(\"key\").cast(\"string\"), from_json(col(\"value\").cast(\"string\"), schema).alias('value'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw JSON Data to archive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Json to HDFS sink with partitioning\n",
    "\n",
    "targetJsonHDFS = df\\\n",
    "    .select(col('key'),col('value.*'))\\\n",
    "    .writeStream\\\n",
    "    .format(\"json\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .partitionBy(\"key\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_archive_path)\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_archive_checkpoint_path)\\\n",
    "    .start()\n",
    "#targetJsonHDFS.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(targetJsonHDFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create  HIVE Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 9870\n",
    "hive_port = 10000\n",
    "hive_username = 'hdfs'\n",
    "hive_password = 'naya'\n",
    "hive_database = 'twitter'\n",
    "hive_mode = 'CUSTOM'\n",
    "\n",
    "hive_cnx = hive.Connection(\n",
    "  host = hdfs_host, \n",
    "  port = hive_port, \n",
    "  username = hive_username,\n",
    "  password = hive_password,\n",
    "  auth = hive_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = f\"drop database if exists {hive_database} cascade \"\n",
    "\n",
    "# with hive_cnx.cursor() as cursor:\n",
    "#     cursor.execute(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = f\"create database if not exists {hive_database}\"\n",
    "\n",
    "with hive_cnx.cursor() as cursor:\n",
    "    cursor.execute(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_cnx = hive.Connection(\n",
    "  host = hdfs_host, \n",
    "  port = hive_port, \n",
    "  username = hive_username,\n",
    "  password = hive_password,\n",
    "  auth = hive_mode,\n",
    "  database=hive_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets to HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"tweets\"\n",
    "colNames = tweet_keys \n",
    "colTypes = [t().simpleString() for t in tweet_types]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colCreate = ', '.join([n + ' ' + t for n,t in zip(colNames, colTypes)])\n",
    "colCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(f\"{hdfs_hive_warehouse}/{hive_database}.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_hive_tweets = f\"{hdfs_hive_warehouse}/{hive_database}.db/{table}\"\n",
    "hdfs_hive_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(hdfs_hive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q =  f\"CREATE EXTERNAL TABLE IF NOT EXISTS {table} ({colCreate}) \\\n",
    "partitioned by (key string)\\n \\\n",
    "STORED AS PARQUET \\\n",
    "LOCATION  '{hdfs_hive_tweets}'\"\n",
    "\n",
    "# f\"LOAD DATA INPATH '{hdfs_hive_staging}/*.parquet' \\\n",
    "# INTO TABLE tweets \\\n",
    "# partition by key \"\n",
    "print(q)\n",
    "\n",
    "#TBLPROPERTIES ('avro.schema.url'='hdfs://$tempdir/avroSchema/$tbl.avsc') ;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hive_cnx.cursor() as cursor:\n",
    "    print(q)\n",
    "    cursor.execute(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump new tweets to HDFS as Parquet (small files):\n",
    "tweets2HIVE = df\\\n",
    "    .select(col('key'),col(f'value.tweets.*'))\\\n",
    "    .writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .partitionBy(\"key\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_hive_tweets)\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_hive_tweets + \"/sparkcheckpoint\" )\\\n",
    "    .start()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(tweets2HIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2HIVE.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2HIVE.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2HIVE.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users to HIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"users\"\n",
    "staging = \"users_staging\"\n",
    "colNames = user_keys[:-1] + [\"last_tweet_at\"]\n",
    "colTypes = [t().simpleString() for t in user_types][:-1] + [StringType().simpleString()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colCreate = ', '.join([n + ' ' + t for n,t in zip(colNames, colTypes)])\n",
    "colCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_hive_users = f\"{hdfs_hive_warehouse}/{hive_database}.db/{table}\"\n",
    "hdfs_hive_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(hdfs_hive_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_hive_users_staging = f\"{hdfs_hive_warehouse}/{hive_database}.db/{staging}\"\n",
    "hdfs_hive_users_staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(hdfs_hive_users_staging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 =  f\"CREATE EXTERNAL TABLE IF NOT EXISTS {table} ({colCreate}) \\\n",
    "partitioned by (key bigint) \\\n",
    "STORED AS PARQUET \\\n",
    "LOCATION '{hdfs_hive_users}'\"\n",
    "\n",
    "# f\"LOAD DATA INPATH '{hdfs_hive_staging}/*.parquet' \\\n",
    "# INTO TABLE tweets \\\n",
    "# partition by key \"\n",
    "print(q1)\n",
    "\n",
    "#TBLPROPERTIES ('avro.schema.url'='hdfs://$tempdir/avroSchema/$tbl.avsc') ;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q2 =  f\"CREATE EXTERNAL TABLE IF NOT EXISTS {staging} ({colCreate}) \\\n",
    "partitioned by (key bigint) \\\n",
    "STORED AS PARQUET \\\n",
    "LOCATION '{hdfs_hive_users_staging}'\"\n",
    "\n",
    "# f\"LOAD DATA INPATH '{hdfs_hive_staging}/*.parquet' \\\n",
    "# INTO TABLE tweets \\\n",
    "# partition by key \"\n",
    "print(q2)\n",
    "\n",
    "#TBLPROPERTIES ('avro.schema.url'='hdfs://$tempdir/avroSchema/$tbl.avsc') ;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hive_cnx.cursor() as cursor:\n",
    "    print(q1)\n",
    "    print(q2)\n",
    "    cursor.execute(q1)\n",
    "    cursor.execute(q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump new users to HDFS as Parquet (small files): .dropDuplicates(subset=['id'])\\\n",
    "users2HIVE = df\\\n",
    "    .select(col(f'value.users.*'),col('key').alias('last_tweet_at'))\\\n",
    "    .writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .partitionBy(\"key\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_hive_users_staging)\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_hive_users_staging + \"/sparkcheckpoint\" )\\\n",
    "    .start()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(users2HIVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2HIVE.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2HIVE.exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users2HIVE.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start HIVE refresher job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "q1 = \"insert into users_staging select * from users where key=2010\"\n",
    "\n",
    "q2 =\"alter table users drop partition (key=2010)\"\n",
    "\n",
    "q3 = \"insert into users partition(key=2010) select * from users_staging\"\n",
    "\n",
    "q4 = \"alter table users_staging drop partition (key=2010)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is probably the right query for the 'merge' of partitions\n",
    "q5 = \"\"\" SELECT id, name, last_tweet_at, followers_count\n",
    "FROM (\n",
    "    SELECT id, name, rank() over (partition by id order by followers_count desc) as last_tweet_at, followers_count\n",
    "    from users_staging\n",
    "    where key =2010\n",
    ") ranked_users\n",
    "WHERE ranked_users.last_tweet_at < 5\n",
    "ORDER BY id, last_tweet_at \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hive_cnx.cursor() as cursor:    \n",
    "    cursor.execute(q1)      \n",
    "    cursor.execute(q2)\n",
    "    cursor.execute(q3)      \n",
    "    cursor.execute(q4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_hive_table(df, epoch_id):\n",
    "    hive_cnx = hive.Connection(\n",
    "        host = 'localhost', \n",
    "        port = 10000, \n",
    "        username = 'hdfs',\n",
    "        password = 'naya',\n",
    "        auth = 'CUSTOM',\n",
    "        database='twitter')\n",
    "    with hive_cnx.cursor() as cursor:    \n",
    "        #cursor.execute(\" MSCK REPAIR TABLE tweets \")      \n",
    "        cursor.execute(\" MSCK REPAIR TABLE users_staging \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVErefresher = df\\\n",
    "                .writeStream\\\n",
    "                .foreachBatch(refresh_hive_table)\\\n",
    "                .trigger( processingTime='1 minute')\\\n",
    "                .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_continiously(HIVErefresher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVErefresher.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVErefresher.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark.SQL Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg = df \\\n",
    "    .select(col(\"value.tweet.*\"))\\\n",
    "    .writeStream \\\n",
    "    .queryName(\"aggregate2\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = spark.sql(\"select * from aggregate2\") # interactively query in-memory table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b  = a.write.parquet(\"hdfs://localhost:8020/tmp/project/tmp.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.show(10,200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"/tmp/project/hive/key=2019-12-25_18-15/part-00000-c5543d3a-7e1d-467d-a0d7-9b8893fe9bb4.c000.snappy.parquet\"\n",
    "c = spark.read.parquet('hdfs://localhost:8020'+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select * from aggregate2\").show(100,1100) # interactively query in-memory table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox \n",
    "below are cells that I played with to test portions of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "options={\"timestampFormat\": \"%a %b %d %H:%M:%S %z %Y\"} #\"yyyy-MM-dd HH:mm:ss\"}\n",
    "options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " schema = StructType().add(\"value\", StructType().add(\"user\", StringType())\n",
    "                                            .add(\"tweet\", StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " schema = StructType().add(\"value\", StructType().add(\"value\", StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, MapType, StructField\n",
    "\n",
    "\n",
    "schema = StructType().add('created_at', StringType(), False).add('id_str', StringType(), False)\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"value\", StringType(), True), True),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"created_at\", StringType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.json(sampleFilePath, schema, multiLine=True)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = spark.read.option(\"multiLine\", True).json(sampleFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsondf = spark.read.json(Seq(jsonstr).toDS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wat = schema.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleFilePath = \"/home/naya/DataEngineerProject/part-00000-7ad71167-a959-4478-929a-6ab7607844d4.c000.json\"\n",
    "sampleFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(sampleFilePath) as f:\n",
    "    txt = f.read()\n",
    "    \n",
    "(txt.__repr__())\n",
    "\n",
    "json.loads(txt.replace('\\\\\"','\\\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleFilePath = \"hdfs://localhost:8020\" +\\\n",
    "                \"/tmp/project/archive/key=2019-12-22_10-18/part-00000-7ad71167-a959-4478-929a-6ab7607844d4.c000.json\"\n",
    "sampleFilePath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sf = sqlContext.read.json(sampleFilePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json to HDFS sink with trigger\n",
    "\n",
    "targetJsonHDFS = df\\\n",
    "    .writeStream\\\n",
    "    .format(\"json\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020\" + hdfs_archive_path)\\\n",
    "    .trigger(processingTime=\"5 seconds\")\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020\" + hdfs_archive_checkpoint_path)\\\n",
    "    .start()\n",
    "targetJsonHDFS.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetJsonHDFS.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet to HDFS sink example with trigger\n",
    "\n",
    "targetParquetHDFS = df\\\n",
    "    .writeStream\\\n",
    "    .format(\"parquet\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"path\", \"hdfs://localhost:8020/tmp/project\")\\\n",
    "    .trigger(processingTime=\"5 seconds\")\\\n",
    "    .option(\"checkpointLocation\", \"hdfs://localhost:8020/tmp/sparkcheckpoint/\")\\\n",
    "    .start()\n",
    "targetParquetHDFS.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetParquetHDFS.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write parquet files to local linux dir\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\")       \\\n",
    "    .option(\"path\", \"/tmp/project\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/sparkcheckpoint/\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write parquet files to local linux dir\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\")       \\\n",
    "    .option(\"path\", \"/tmp/project\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/sparkcheckpoint/\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output to console\n",
    "# query = df \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "# query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = df.select(\n",
    "#    explode(\n",
    "#        split(df.value, \" \")\n",
    "#    ).alias(\"word\")\n",
    "# )\n",
    "\n",
    "# Generate running word count\n",
    "#wordCounts = words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = wordCounts \\\n",
    "#     .writeStream \\\n",
    "#     .outputMode(\"complete\") \\\n",
    "#     .format(\"console\") \\\n",
    "#     .start()\n",
    "# query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
