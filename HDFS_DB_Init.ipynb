{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "from common_vars import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#USE WITH CAUTION\n",
    "\n",
    "#DELETE and recreate the data and checkpoint directories in hdfs \n",
    "\n",
    "\n",
    "\n",
    "def reset_dir(hdfs_path):\n",
    "    if fs.exists(hdfs_path):\n",
    "        fs.delete(hdfs_path,recursive=True)\n",
    "        print(\"directory deleted: \" + hdfs_path)\n",
    "    else:\n",
    "        print(\"no such dir: \" + hdfs_path)\n",
    "\n",
    "    fs.mkdir(hdfs_path + '/sparkcheckpoint')    \n",
    "    print(\"directory created: \" + fs.ls(hdfs_path)[0])\n",
    "    fs.chmod(hdfs_path + '/sparkcheckpoint', 0o777)   \n",
    "    fs.chmod(hdfs_path, 0o777)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#queries must be list of strings [\"select from...\", \"insert into...\" ]\n",
    "def run_queries(queries):\n",
    "    with hive_cnx.cursor() as cursor:   \n",
    "        for q in queries:\n",
    "            cursor.execute(q)    \n",
    "        if cursor.poll().hasResultSet:\n",
    "            return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(hdfs_archive_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(hdfs_hive_staging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create  HIVE Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_cnx = hive.Connection(\n",
    "  host = hdfs_host, \n",
    "  port = hive_port, \n",
    "  username = hive_username,\n",
    "  password = hive_password,\n",
    "  auth = hive_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = f\"drop database if exists {hive_database} cascade \"\n",
    "\n",
    "# with hive_cnx.cursor() as cursor:\n",
    "#     cursor.execute(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(f\"{hdfs_hive_warehouse}/{hive_database}.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_queries([f\"create database if not exists {hive_database}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_cnx = hive.Connection(\n",
    "  host = hdfs_host, \n",
    "  port = hive_port, \n",
    "  username = hive_username,\n",
    "  password = hive_password,\n",
    "  auth = hive_mode,\n",
    "  database=hive_database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tweets table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"tweets\"\n",
    "colNames = tweet_keys.copy()\n",
    "colNames += [\"Sentiment\",\"n_words\"]\n",
    "\n",
    "colTypes = [t().simpleString() for t in tweet_types]\n",
    "colTypes += ['int','int']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(colNames,colTypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colCreate = ', '.join([n + ' ' + t for n,t in zip(colNames, colTypes)])\n",
    "colCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(hdfs_hive_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q =  f\"CREATE EXTERNAL TABLE IF NOT EXISTS {table} ({colCreate}) \\\n",
    "partitioned by (key string)\\n \\\n",
    "STORED AS PARQUET \\\n",
    "LOCATION  '{hdfs_hive_tweets}'\"\n",
    "\n",
    "# f\"LOAD DATA INPATH '{hdfs_hive_staging}/*.parquet' \\\n",
    "# INTO TABLE tweets \\\n",
    "# partition by key \"\n",
    "print(q)\n",
    "\n",
    "#TBLPROPERTIES ('avro.schema.url'='hdfs://$tempdir/avroSchema/$tbl.avsc') ;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_queries([q])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Users Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"users\"\n",
    "staging = \"users\" # must actually be \"users_staging\", but the aggregation strategy does not work (see below) \n",
    "colNames = ['last_tweet_at'] + user_keys[:-1]\n",
    "colTypes = [t().simpleString() for t in [StringType] + user_types][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colCreate = ', '.join([n + ' ' + t for n,t in zip(colNames, colTypes)])\n",
    "colCreate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(hdfs_hive_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_dir(hdfs_hive_users_staging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 =  [f\"CREATE EXTERNAL TABLE IF NOT EXISTS {table} ({colCreate}) \\\n",
    "partitioned by ({partitionCol} string) \\\n",
    "STORED AS PARQUET \\\n",
    "LOCATION '{hdfs_hive_users}'\"]\n",
    "\n",
    "# f\"LOAD DATA INPATH '{hdfs_hive_staging}/*.parquet' \\\n",
    "# INTO TABLE tweets \\\n",
    "# partition by key \"\n",
    "print(q1)\n",
    "\n",
    "#TBLPROPERTIES ('avro.schema.url'='hdfs://$tempdir/avroSchema/$tbl.avsc') ;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 +=  [f\"CREATE TABLE IF NOT EXISTS {staging} ({colCreate}) \\\n",
    "partitioned by ({partitionCol} string) \\\n",
    "STORED AS PARQUET \\\n",
    "LOCATION '{hdfs_hive_users_staging}'\"]\n",
    "\n",
    "# f\"LOAD DATA INPATH '{hdfs_hive_staging}/*.parquet' \\\n",
    "# INTO TABLE tweets \\\n",
    "# partition by key \"\n",
    "print(q1)\n",
    "\n",
    "#TBLPROPERTIES ('avro.schema.url'='hdfs://$tempdir/avroSchema/$tbl.avsc') ;\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_queries(q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, MapType, StructField,\\\n",
    "                              BooleanType, DateType, NumericType, IntegerType,\\\n",
    "                              LongType, TimestampType, FloatType, ArrayType\n",
    "import pyarrow as pa\n",
    "\n",
    "\n",
    "topic = 'TweeterArchive'\n",
    "partitionCol = \"created_ym\"\n",
    "Keywords = 'Israel'\n",
    "\n",
    "\n",
    "hdfs_host = 'localhost'\n",
    "hdfs_port = 9870\n",
    "hive_port = 10000\n",
    "hive_username = 'hdfs'\n",
    "hive_password = 'naya'\n",
    "hive_database = 'twitter'\n",
    "hive_mode = 'CUSTOM'\n",
    "\n",
    "\n",
    "fs = pa.hdfs.connect(\n",
    "    host=hdfs_host, \n",
    "    port=8020, \n",
    "    user=hive_username, \n",
    "    kerb_ticket=None, \n",
    "    driver='libhdfs', \n",
    "    extra_conf=None)\n",
    "\n",
    "\n",
    "event_fields = [ 'id', 'text','created_at', 'geo', 'coordinates', 'place',\n",
    "                 'quote_count', 'reply_count', 'retweet_count', 'favorite_count' ]\n",
    "\n",
    "tweet_keys =  event_fields + ['user_id', 'user_followers' ]\n",
    "\n",
    "tweet_types = [LongType, StringType, TimestampType, StringType, StringType, StringType, \n",
    "               IntegerType, IntegerType, IntegerType, IntegerType, LongType, IntegerType]\n",
    "\n",
    "# tweet_types  = [StringType]* 11 \n",
    "\n",
    "\n",
    "user_fields = ['id', 'name', 'screen_name','created_at', 'location', 'url',\n",
    "                         'protected', 'verified', 'followers_count', 'friends_count',\n",
    "                         'listed_count', 'favourites_count', 'statuses_count', 'withheld_in_countries']\n",
    "\n",
    "user_keys = user_fields + [partitionCol]\n",
    "\n",
    "user_types = [LongType, StringType, StringType, TimestampType, StringType, StringType, \n",
    "              BooleanType, BooleanType, IntegerType, IntegerType, \n",
    "              IntegerType, IntegerType, IntegerType, StringType, StringType]\n",
    "\n",
    "# user_types = [StringType]* 14\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
